---
apiVersion: batch/v1
kind: Job
metadata:
  name: create-buckets
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  backoffLimit: 4
  template:
    spec:
      serviceAccount: minio-manage
      serviceAccountName: minio-manage
      initContainers:
      - name: wait-for-minio
        image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        args:
        - -ec
        - |-
          echo -n "Waiting for minio pod in shared-minio namespace"
          while [ -z "$(oc get pod -n shared-minio -l app=minio -o name 2>/dev/null)" ]; do
              echo -n '.'
              sleep 1
          done
          echo "Minio pod is running in shared-minio namespace"
      containers:
      - name: add-model
        image: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-generic-data-science-notebook:1.2
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash"]
        args:
        - -ec
        - |-
          #curl -LO https://rhods-public.s3.amazonaws.com/demo-models/ic-models/accident/accident_detect.onnx
          #curl -LO https://rhods-public.s3.us-east-1.amazonaws.com/wildfire/wildfire_types_image_detection_model01.zip
          #unzip wildfire_types_image_detection_model01.zip
          mkdir -p wildfire_onnx
          cd wildfire_onnx
          time curl -LO https://rhods-public.s3.us-east-1.amazonaws.com/wildfire/wildfire_onnx.zip
          time unzip wildfire_onnx.zip
          rm wildfire_onnx.zip
          cd ..

          time cat << 'EOF' | python3
          import boto3, os

          s3 = boto3.client("s3",
                            endpoint_url=os.getenv("AWS_S3_ENDPOINT"),
                            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
                            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"))

          # Create user buckets
          for i in range(1, 101):
              bucket_name = f"user{i}"
              if bucket_name not in [bu["Name"] for bu in s3.list_buckets()["Buckets"]]:
                  s3.create_bucket(Bucket=bucket_name)

          # Create the models bucket
          models_bucket_name = "models"
          if models_bucket_name not in [bu["Name"] for bu in s3.list_buckets()["Buckets"]]:
              s3.create_bucket(Bucket=models_bucket_name)

          # uploading zip to minio
          #filename = "wildfire_types_image_detection_model01.zip"
          #with open(filename, "rb") as f:
          #    s3.upload_fileobj(f, "models", f'wildfire/1/{filename}')

          # Directory to upload
          directory = "./wildfire_onnx/"
          # directory = "./wildfire_types_image_detection_model01/"
          bucket_name = "models"
          base_s3_path = "wildfire_onnx/1/"
          # base_s3_path = "wildfire/1/"

          # Iterate over all files in the directory
          for root, dirs, files in os.walk(directory):
              for file in files:
                  # Construct the full file path
                  file_path = os.path.join(root, file)

                  # Construct the S3 object key (path within the bucket)
                  relative_path = os.path.relpath(file_path, directory)
                  s3_key = os.path.join(base_s3_path, relative_path)

                  # Upload the file
                  with open(file_path, "rb") as f:
                      s3.upload_fileobj(f, bucket_name, s3_key)

                  print(f"Uploaded {file_path} to {bucket_name}/{s3_key}")
          EOF
        envFrom:
        - secretRef:
            name: aws-connection-minio
      restartPolicy: Never
# ---
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: add-images
#   annotations:
#     argocd.argoproj.io/sync-wave: "3"
#     argocd.argoproj.io/hook: Sync
#     argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
# spec:
#   backoffLimit: 4
#   template:
#     spec:
#       serviceAccount: minio-manage
#       serviceAccountName: minio-manage
#       initContainers:
#       - name: wait-for-minio
#         image: image-registry.openshift-image-registry.svc:5000/openshift/tools:latest
#         imagePullPolicy: IfNotPresent
#         command: ["/bin/bash"]
#         args:
#         - -ec
#         - |-
#           echo -n "Waiting for minio pod in shared-minio namespace"
#           while [ -z "$(oc get pod -n shared-minio -l app=minio -o name 2>/dev/null)" ]; do
#               echo -n '.'
#               sleep 1
#           done
#           echo "Minio pod is running in shared-minio namespace"
#       containers:
#       - name: add-model
#         image: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-generic-data-science-notebook:1.2
#         imagePullPolicy: IfNotPresent
#         command: ["/bin/bash"]
#         args:
#         - -ec
#         - |-
#           mkdir -p images
#           cd images
#           time curl -LO https://rhods-public.s3.us-east-1.amazonaws.com/wildfire/wildfire_types_image_detection_training01.zip

#           time wildfire_types_image_detection_training01.zip
#           rm wildfire_types_image_detection_training01.zip
#           ls -altr

#           cd ..

#           time cat << 'EOF' | python3
#           import boto3, os

#           s3 = boto3.client("s3",
#                             endpoint_url=os.getenv("AWS_S3_ENDPOINT"),
#                             aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
#                             aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"))

#           # Create the images bucket
#           images_bucket_name = "images"
#           if images_bucket_name not in [bu["Name"] for bu in s3.list_buckets()["Buckets"]]:
#               s3.create_bucket(Bucket=images_bucket_name)

#           # uploading zip to minio
#           #filename = "wildfire_types_image_detection_model01.zip"
#           #with open(filename, "rb") as f:
#           #    s3.upload_fileobj(f, "images", f'wildfire/1/{filename}')

#           # Directory to upload
#           directory = "./images/"
#           bucket_name = "images"
#           base_s3_path = "images/"

#           # Iterate over all files in the directory
#           for root, dirs, files in os.walk(directory):
#               for file in files:
#                   # Construct the full file path
#                   file_path = os.path.join(root, file)

#                   # Construct the S3 object key (path within the bucket)
#                   relative_path = os.path.relpath(file_path, directory)
#                   s3_key = os.path.join(base_s3_path, relative_path)

#                   # Upload the file
#                   with open(file_path, "rb") as f:
#                       s3.upload_fileobj(f, bucket_name, s3_key)

#                   print(f"Uploaded {file_path} to {bucket_name}/{s3_key}")
#           EOF
#         envFrom:
#         - secretRef:
#             name: aws-connection-minio
#       restartPolicy: Never
